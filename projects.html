<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="personal_website" content="">
    <meta name="Mallikarjun B R" content="">

    <title>Mallikarjun B R</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/blog-home.css" rel="stylesheet">

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Load Navigation bar -->
    <script> 
        $(function(){
            $("#nav_bar").load("nav.html"); 
        });
    </script> 

</head>

<body>

    <!-- Navigation bar -->
    <div id="nav_bar"> </div>
    <!-- Navigation bar -->

    <!-- Page Content -->
    <div class="container">

        <div class="row">
            
            <!-- Research Projects -->
            <h2>Research Projects</h2><hr>
                
            <!-- Face Fiducial Detection -->
            <h3>Face Fiducial Detection</h3>

            <div class="row">
                <div class="col-md-8">
                    <p>
                        Facial fiducial detection is a challenging problem for several reasons like varying pose, appearance, expression, partial occlusion and others. In the past, several approaches like mixture of trees, regression based methods, exemplar based methods have been proposed to tackle this challenge. In this project, we propose an exemplar based approach to select the best solution from among outputs of regression and mixture of trees based algorithms (which we call candidate algorithms). We show that by using a very simple SIFT and HOG based descriptor, it is possible to identify the most accurate fiducial outputs from a set of results produced by candidate algorithms on any given test image. Our approach manifests as two algorithms, one based on optimizing an objective function with quadratic terms and the other based on simple kNN. Both algorithms take as input fiducial locations produced by running state-of-the-art candidate algorithms on an input image, and output accurate fiducials using a set of automatically selected exemplar images with annotations. Our surprising result is that in this case, a simple algorithm like kNN is able to take advantage of the seemingly huge complementarity of these candidate algorithms, better than optimization based algorithms. We do extensive experiments on several datasets, and show that our approach outperforms state-of-the-art consistently. In some cases, we report as much as a 17% improvement in accuracy. We also extensively analyze each component of our approach, to illustrate its efficacy.
                    </p>
                    <small>
                        <a href="https://cvit.iiit.ac.in/research/projects/cvit-projects/face-fiducial-detection-by-consensus-of-exemplars">[Project Page]</a>
                        <a href="https://cvit.iiit.ac.in/images/Projects/Fiducialexemplar/wacv_fiducial.pdf">[PDF]</a>
                        <a href="https://bitbucket.org/mallikarjun26/wacv_2016_face_fiducial">[Code]</a>
                    </small>
                </div>
                <div class="col-md-4">
                    <center>
                        <img src="figures/face_fid_image.png" height="250" width="290">
                    </center>
                </div>
            </div>
            <!-- Face Fiducial Detection -->

            <!-- Efficient Face Frontalization in Unconstrained Images -->
            <h3>Efficient Face Frontalization in Unconstrained Images</h3>
            <div class="row">
                <div class="col-md-8">
                    <p>
                        Face frontalization is the process of synthesizing a frontal view of a face, given its non-frontal view. Frontalization is used in intelligent photo editing tools and also aids in improving the accuracy of face recognition systems. For example, in the case of photo editing, faces of persons in a group photo can be corrected to look into the camera, if they are looking elsewhere. Even though recent methods in face recognition claim accuracy which surpasses that of humans in some cases, performance of recognition systems degrade when profile view of faces are given as input. One way to address this issue is to synthesize frontal views of faces before recognition.
                        We propose a simple and efficient method to address the face frontalization problem. Our method leverages the fact that faces in general have a definite structure and can be represented in a low dimensional subspace. We employ an exemplar based approach to find the transformation that relates the profile view to the frontal view, and use it to generate realistic frontalizations. Our method does not involve estimating 3D model of the face, which is a common approach in previous work in this area. This leads to an efficient solution, since we avoid the complexity of adding one more dimension to the problem. Our method also retains the structural information of the individual as compared to that of a recent method, which assumes a generic 3D model for synthesis. We show impressive qualitative and quantitative results in comparison to the state-of-the-art in this field.
                    </p>
                    <small>
                        <a href="https://researchweb.iiit.ac.in/~mallikarjun.br/data/frontalization_2015.pdf">[PDF]</a>
                        <a href="">[Code]</a>
                    </small>
                </div>

                <div class="col-md-4">
                    <center>
                        <img src="figures/face_front.png" height="100" width="180">
                        <video width="320" height="240" controls>
                            <source src="figures/face_front_demo.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </center>
                </div>
            </div>
            <!-- Efficient Face Frontalization in Unconstrained Images -->

            <!-- Face Video Synthesis -->
            <h3>Face Video Synthesis</h3>
            <div class="row">
                <div class="col-md-8">
                    <p>
                        Consider face space of a particular individual. Assume it contains all possible variations in pose and expression dimensions. This project looks at the possibility of synthesizing novel videos which would have the person changing his expression and pose in smooth transition. We created a dataset of 10,000 images of Jack Nicholson's faces extracted from <i>As Good As it Gets</i> movie. Face space is represented as a network of nodes and edges representing face and properties respectievely. Property defining edge between two faces would include appearance features like SIFT and HoG and structural features like the locations of facial landmarks. Given this network, we explored various type of traversals to synthesize novel videos.
                        Second part of the project looked at the possibility of factorizing a given face into individual and expression components. Also, could the non-individual component be transferred to another individual's face to synthesize a new image which imitates the former's expression. Aligned face images of the individual are stacked to form a 3D tensor. 2D tensor across all the faces are used to obtain Eigen vectors. Experiments were carried out to identity components responsible for individuality compared to other vectors responsible for subtle facial features. 
                    </p>
                    <small>
                    </small>
                </div>
                <div class="col-md-4">
                    <center>
                        <video width="150" height="150" controls>
                            <source src="figures/face_synth.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </center>
                </div>
            </div>
            <!-- Face Video Synthesis -->

            <!-- Research Projects -->

            <!-- Course Projects -->
            <h2>Course Projects</h2> <hr>

            <!-- Sign Language Detection -->
            <h3>Sign Language Detection</h3>
            <div class="row">
                <div class="col-md-8">
                    <p>
                        English characters are represented as signs using hand gestures. This the way of communication for people who can not speak. This project aims at classifying the signs in the form of static image using Convolutional Neural Networks. Dataset consists of 700 train images and 300 test images. We performed various experiments with varying feauture representation (in the form of RGB, YUV and canny transformed image), adding augmented data (which includes rotation, translation, horizontal flip and noise), various architectures (varying depth, number of feature maps, Stochastic and Batch Gradient descent and Dropout).<br>
                    </p>
                    <p>
                        Results were compared to that of <i>Bag of Visual Words</i> model. 600 visual words were used to build the histogram representation of each image. SVM was used as classifier. This model achieved an accuracy of 25.32% as compared to that of CNN which out performed with an accuracy of 63% with the best performing parameters. We observed that the data augmentation played a crucial role in improving the accuracy of the system significantly.                      
                    </p>
                    <small>
                        <a href="https://github.com/mallikarjun26/SignLanguageRecognition">[Code]</a>
                    </small>
                </div>
                <div class="col-md-4">
                    <center>
                        <img src="figures/sign_lang.png" height="150" width="260">
                    </center>
                </div>
            </div>
            <!-- Sign Language Detection -->

            <!-- Document Layout Analysis -->
            <h3>Document Layout Analysis</h3>
            <div class="row">
                <div class="col-md-8">
                    <p>
                        Documents consists of various components, such as headings, text content, tabels, images and graphs. This project aims to automatically segment and classify components for tagging. In this project we consider IEEE standard techinical documents in image format. Document is segmented in to multiple components by employing morphological processes such as dilation with suitable structural elements. Each of this segmented component now is considered for labelling. 
                    </p>
                    <p>
                        Scores based on location, statistical measure of connected components with in the boundary, number of parallel lines based on hough transform is assigned to each segmented component. Based on the scores one of the following labels(heading, text block, table, figure and equation) has been assigned. We experimented with various thresholds to achieve best performance. Achieved an accuracy of 80% in labelling the components.   
                    </p>
                    <small>
                        <a href="data/DocumentLayoutAnalysis.pdf">[Slides]</a>
                    </small>
                </div>
                <div class="col-md-4">
                    <center>
                        <img src="figures/doc_anly.png" height="150" width="260">
                    </center>
                </div>
            </div>
            <!-- Document Layout Analysis -->

            <!-- Lead Character Recognition in a Movie -->
            <h3>Lead Character Recognition in a Movie</h3>
            <div class="row">
                <div class="col-md-8">
                    <p>
                         Given a movie, this project detects the protagonists. We make an assumption that the movie revolves around the main characters and screen is occupied by these characters most of the time. All the faces are extracted from the movie using Zhu <i>et al.</i><a href="https://www.ics.uci.edu/~xzhu/face/"></a> face detector instead of OpenCV detector as it performs better in non-frontal face view as well. Appearance of each of these faces is represented as Local Binary Pattern (LBP) feature vector. Faces are clustered with each cluster representing each character in the movie using Hierarchial clustering. Based on number of samples assigned to the clusters, lead characters are chosen.
                    </p>
                    <small>
                    </small>
                </div>
                <div class="col-md-4">
                    <center>
                        <img height="150" width="150" src="figures/cersei.jpg">
                        <img height="150" width="150" src="figures/johnsnow.jpg">
                    </center>
                </div>
            </div>
            <!-- Lead Character Recognition in a Movie -->

            <!-- Twitter Entity Disambiguation -->
            <h3>Twitter Entity Disambiguation</h3>
            <div class="row">
                <div class="col-md-8">
                    <p>
                        A major problem in monitoring the online reputation of companies, brands, and other entities is that entity names are often ambiguous (apple may refer to the company, the fruit, the singer, etc.). The problem is particularly hard in microblogging services such as Twitter, where texts are very short and there is little context to disambiguate. In this project, we address the filtering task of determining, out of a set of tweets that contain a company name, which ones do refer to the company. To address this task we studied a large set of features that can be generated to describe the relationship between an entity and a tweet. We explored different learning algorithms as well as, different types of features: text, keyword similarity scores between entities metadata and tweets, Freebase entity graph and Wikipedia. The corpus consists of tweets and a list of 61 entities. For each tweet in the corpus we have the target entity id, the language of the tweet, the timestamp and the tweet id. The content of each URL in the tweets is also provided. The Wikipedia page and home page of the entity are also included.
                    </p>
                    <small>
                    </small>
                </div>
                <div class="col-md-4">
                </div>
            </div>
            <!-- Twitter Entity Disambiguation -->

            <!-- Search Engine for Wikipedia -->
            <h3>Search Engine for Wikipedia</h3>
            <div class="row">
                <div class="col-md-8">
                    <p>
                        Text based search engine was developed. It involved pre-processing of 43GB of Wikipedia corpus and building an inverted index. Retrieved documents for query were ranked based on tf-idf model.
                    </p>
                    <small>
                    </small>
                </div>
                <div class="col-md-4">
                </div>
            </div>
            <!-- Search Engine for Wikipedia -->

            <!-- Path Planning for 3 Link Manipulator using Rapidly Exploring Random Tree -->
            <h3>Path Planning for 3 Link Manipulator using Rapidly Exploring Random Tree</h3>
            <div class="row">
                <div class="col-md-8">
                    <p>
                        Project aims to find the path between two point in space for a 3 Link Manipulator using RRT in the presence of obstacles. Kinematic model of the robot has been developed. RRT has been performed on the configuration space to build the path.
                    </p>
                    <small>
                    </small>
                </div>
                <div class="col-md-4">
                </div>
            </div>
            <!-- Path Planning for 3 Link Manipulator using Rapidly Exploring Random Tree -->

            <!-- Course Projects -->
            
            <!-- Bachelors Projects -->
            <h2>Bachelors Project</h2> <hr>

            <!-- Mobile controlled robot -->
            <h3>Mobile controlled robot</h3>
            <div class="row">
                <div class="col-md-8">
                    <p>
                        Built a robot with a robotic arm (clipper). Operations of the robot are controlled using mobile phone. Each of the key is configured to perform certain action. Whenever a key is pressed, a Dual Tone Multiple Frequency (DTMF) signal is sent over the network. This signal is decoded at the robot end and the corresponding action is performed.
                    </p>
                    <small>
                    </small>
                </div>
                <div class="col-md-4">
                </div>
            </div>
            <!-- Mobile controlled robot -->

            <!-- Bachelors Projects -->

            <hr>

        <div>

        <hr>

    </div>
    <!-- /.container -->

</body>

</html>
